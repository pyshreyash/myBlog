<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Implementing Word2Vec from scratch (including the backpropogation) | Shreyash&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Python and Numpy based implementation for the Word2Vec (Mikolov et al)
Well vector embeddings play crucial role in deep learning as modern neural nets feed on them. In order to apply neural networks on NLP tasks we need to crunch down the language/words to numbers. There&rsquo;s where word embedding come in play. [link] explains more about this concepts and how they if you are new to the field.
$$ x=1 $$\[
f(x) = x^2 &#43; x &#43; 1\]This article is under development">
<meta name="author" content="">
<link rel="canonical" href="https://pyshreyash.github.io/myBlog/docs/test/">
<link crossorigin="anonymous" href="/myBlog/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk&#43;4bvpN&#43;sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://pyshreyash.github.io/myBlog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://pyshreyash.github.io/myBlog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://pyshreyash.github.io/myBlog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://pyshreyash.github.io/myBlog/apple-touch-icon.png">
<link rel="mask-icon" href="https://pyshreyash.github.io/myBlog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://pyshreyash.github.io/myBlog/docs/test/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>
  <meta property="og:url" content="https://pyshreyash.github.io/myBlog/docs/test/">
  <meta property="og:site_name" content="Shreyash&#39;s Blog">
  <meta property="og:title" content="Implementing Word2Vec from scratch (including the backpropogation)">
  <meta property="og:description" content="Python and Numpy based implementation for the Word2Vec (Mikolov et al) Well vector embeddings play crucial role in deep learning as modern neural nets feed on them. In order to apply neural networks on NLP tasks we need to crunch down the language/words to numbers. There’s where word embedding come in play. [link] explains more about this concepts and how they if you are new to the field.
$$ x=1 $$\[ f(x) = x^2 &#43; x &#43; 1\]This article is under development">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2024-11-22T14:28:01+05:30">
    <meta property="article:modified_time" content="2024-11-22T14:28:01+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Implementing Word2Vec from scratch (including the backpropogation)">
<meta name="twitter:description" content="Python and Numpy based implementation for the Word2Vec (Mikolov et al)
Well vector embeddings play crucial role in deep learning as modern neural nets feed on them. In order to apply neural networks on NLP tasks we need to crunch down the language/words to numbers. There&rsquo;s where word embedding come in play. [link] explains more about this concepts and how they if you are new to the field.
$$ x=1 $$\[
f(x) = x^2 &#43; x &#43; 1\]This article is under development">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Docs",
      "item": "https://pyshreyash.github.io/myBlog/docs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Implementing Word2Vec from scratch (including the backpropogation)",
      "item": "https://pyshreyash.github.io/myBlog/docs/test/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Implementing Word2Vec from scratch (including the backpropogation)",
  "name": "Implementing Word2Vec from scratch (including the backpropogation)",
  "description": "Python and Numpy based implementation for the Word2Vec (Mikolov et al) Well vector embeddings play crucial role in deep learning as modern neural nets feed on them. In order to apply neural networks on NLP tasks we need to crunch down the language/words to numbers. There\u0026rsquo;s where word embedding come in play. [link] explains more about this concepts and how they if you are new to the field.\n$$ x=1 $$\\[ f(x) = x^2 + x + 1\\]This article is under development",
  "keywords": [
    
  ],
  "articleBody": "Python and Numpy based implementation for the Word2Vec (Mikolov et al) Well vector embeddings play crucial role in deep learning as modern neural nets feed on them. In order to apply neural networks on NLP tasks we need to crunch down the language/words to numbers. There’s where word embedding come in play. [link] explains more about this concepts and how they if you are new to the field.\n$$ x=1 $$\\[ f(x) = x^2 + x + 1\\]This article is under development ",
  "wordCount" : "82",
  "inLanguage": "en",
  "datePublished": "2024-11-22T14:28:01+05:30",
  "dateModified": "2024-11-22T14:28:01+05:30",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://pyshreyash.github.io/myBlog/docs/test/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shreyash's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://pyshreyash.github.io/myBlog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://pyshreyash.github.io/myBlog/" accesskey="h" title="Shreyash&#39;s Blog (Alt + H)">Shreyash&#39;s Blog</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://pyshreyash.github.io/myBlog/">Home</a>&nbsp;»&nbsp;<a href="https://pyshreyash.github.io/myBlog/docs/">Docs</a></div>
    <h1 class="post-title entry-hint-parent">
      Implementing Word2Vec from scratch (including the backpropogation)
    </h1>
    <div class="post-meta"><span title='2024-11-22 14:28:01 +0530 +0530'>November 22, 2024</span>

</div>
  </header> 
  <div class="post-content"><h3 id="python-and-numpy-based-implementation-for-the-word2vec-mikolov-et-al">Python and Numpy based implementation for the Word2Vec (Mikolov et al)<a hidden class="anchor" aria-hidden="true" href="#python-and-numpy-based-implementation-for-the-word2vec-mikolov-et-al">#</a></h3>
<p>Well vector embeddings play crucial role in deep learning as modern neural nets feed on them. In order to apply neural networks on NLP tasks we need to crunch down the language/words to numbers. There&rsquo;s where word embedding come in play. [link] explains more about this concepts and how they if you are new to the field.</p>
$$ x=1 $$\[
f(x) = x^2 + x + 1\]<h1 id="this-article-is-under-development">This article is under development<a hidden class="anchor" aria-hidden="true" href="#this-article-is-under-development">#</a></h1>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>
